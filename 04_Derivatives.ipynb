{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Производная, дифференцирование.\n",
    "\n",
    "[Лекция \"Предел и производная\"](https://docs.google.com/presentation/d/e/2PACX-1vQrmFcOnz5M_88Hg4XD_hsP7AYgiwGcFl14JZfKo8Cqv8wts1Gj8_Ebd4fP7_zKhso32dE9HvSYQuYn/pub?start=false&loop=false&delayms=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo\n",
    "\n",
    "- производная для умножения, производная степенной функции\n",
    "- добавить информацию про y.grad_fn\n",
    "- squeeze, unsqueeze, reshape, resize, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Производные в torch.Tensor (отличие от numpy.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличительной способностью torch является возможность автоматически рассчитывать производные. Каждый тензор, который вы создаете, способен \"запоминать\" какие действия с ним происходили (перемножение, сложение, возведение в степень), с той целью, чтобы при вызове метода .grad( ) рассчитать значение производной для этого тензора. По умолчанию тензор не ведет запись операций, чтобы изменить это, надо установить значения свойства requires_grad в True. \n",
    "\n",
    "Забегая немного вперед, скажем, что во время вычислений с тензором, у которого флаг requires_grad установлен в  True, для него строится вычислительный граф, который даже можно изобразить с помощью дополнительной библиотеки, но об этом позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Производная для одной переменной\n",
    "\n",
    "Давайте посмотрим как можно пользоваться автоматическим вычислением производной в torch. Для начала импортируем требуемые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создадим одномерный тензор длинной один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "убедимся, что сейчас тензор не будет \"запоминать\" происходящих с ним операций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "установим флаг requires_grad в True, для того, чтобы затем можно было рассчитать производную для этого тензора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "произведем вычисления, включающие в себя операции с нашей переменной x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "значение производной хранится в свойстве grad и изначально равно None чтобы рассчитать его, надо вызвать соответствующий метод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "метод, который вычисляет производные, называется backward() - что можно перевести как \"обратное направление\". Называется он так, потому что производные вычисляются из конца в начало и вызывать его надо от тензора, который является результатом функции: в нашем случае от тензора y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "теперь в grad хранится значение производной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Производная для многомерной функции\n",
    "\n",
    "В предыдущем примере значение переменной **y** зависело всего от одной переменной **x**, поэтому, вызывая метод **y.backward( )**, производная была вычислена только лишь для **x**. Как вы могли догадаться, если **y** будет зависеть от большего числа переменных, то при вызове **y.bacward( )** производные расчитаются и для всех них. Давайте проверим это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 = torch.Tensor([3])\n",
    "x_2 = torch.Tensor([5])\n",
    "x_3 = torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1.requires_grad = True\n",
    "x_2.requires_grad = True\n",
    "x_3.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = -5*x_1 + 2*x_2 + 7*x_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-5.]), tensor([2.]), tensor([7.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.grad, x_2.grad, x_3.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Что вам напоминает вычисление этой функции?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([3, 5, 1])\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = torch.Tensor([-5, 2, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x.dot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.,  2.,  7.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "Получается мы можем вычислить производную и для вектора, просто по-отдельности посчитав производные для его компонент. Но вектор - частный случай матрицы, а матрица - частный случай тензора - значит можно вычислять производные и для них. И даже нужно, если мы хотим обучить нейросеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.Tensor([1, 2, -1])\n",
    "\n",
    "m = torch.Tensor([ \n",
    "    [ 1, -1,  0],\n",
    "    [ 0,  5, -4],\n",
    "    [-2,  0,  2]\n",
    "])\n",
    "\n",
    "p = torch.Tensor([1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемножим вектор v на матрицу m, после этого результат скалярно умножим на вектор p. Но если не изменить размерности векторов, мы не сможем совершить умножение. В torch матрицу можно умножить только на матрицу, вектор на вектор. Вектор отличается от матрицы тем, что у него лишь одна размерность, это можно увидеть, вызвав свойство shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) - это форма вектора v, видим, что он одномерный (длинна равна 3)\n",
      "torch.Size([3, 3]) - это форма матрицы m, видим, что она многомерна (высота и ширины равны 3)\n"
     ]
    }
   ],
   "source": [
    "print(v.shape, '- это форма вектора v, видим, что он одномерный (длинна равна 3)')\n",
    "print(m.shape, '- это форма матрицы m, видим, что она многомерна (высота и ширины равны 3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать из вектора матрицу с одной строкой, можно вызвать метод **.unsqueeze(dim)** от тензора, dim - какая размность будет являться фиктивной. Unsqueeze можно перевести как \"расжать\", то есть увеличить размерность. С его помощью можно сделать из одномерного массива двумерный, из двумерного трехмерный или из трехмерного сразу стомерный, добавляя \"фиктивные\" размерности. Работает это так: пусть у нас есть одномерный массив\n",
    "\n",
    "```Python\n",
    "a = [1,2,3]\n",
    "```\n",
    "\n",
    "мы можем легко сделать из него двумерный, добавив еще больше скобочек\n",
    "\n",
    "\n",
    "```Python\n",
    "a = [[1,2,3]]\n",
    "```\n",
    "\n",
    "но ведь можно сделать двумерный массив и другим образом, сделав матрицу с одним столбцом и тремя строками\n",
    "\n",
    "```Python\n",
    "a = [[1],[2],[3]]\n",
    "```\n",
    "\n",
    "в методе **.unsqueeze(dim)** параметр dim как раз указывает каким образом увеличить размерность (какую размерность сделать фиктивной: первую, вторую или третью и тд).\n",
    "\n",
    "\n",
    "Метод-антагонист называется как неудивительно **squeeze( )**. Он сжимает размерности, но об этом позже. Вернемся к нашей задаче, мы хотим умножить v на матрицу m. Для этого сделаем из вектора v матрицу, добавив фиктивную размерность и превратим v в матрицу с одной строкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = v.unsqueeze(0).requires_grad_(True)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = p.unsqueeze(1).requires_grad_(True)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемножим v на m и результат на p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = v.mm(m).mm(p)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -4.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.],\n",
       "        [  9.],\n",
       "        [-10.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(input_shape, 64, bias=True)\n",
    "        self.fc2 = torch.nn.Linear(64, 32, bias=True)\n",
    "        self.fc3 = torch.nn.Linear(32, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MyModel(192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.rand(16, 192)\n",
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = sum(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3304, -0.4305, -0.2893,  ..., -0.3323, -0.3678, -0.3558],\n",
       "        [ 0.3325,  0.4333,  0.2912,  ...,  0.3345,  0.3702,  0.3582],\n",
       "        [ 0.1751,  0.2281,  0.1533,  ...,  0.1761,  0.1949,  0.1886],\n",
       "        ...,\n",
       "        [-0.4805, -0.6262, -0.4208,  ..., -0.4834, -0.5350, -0.5176],\n",
       "        [ 0.6037,  0.7867,  0.5287,  ...,  0.6073,  0.6721,  0.6502],\n",
       "        [ 0.0487,  0.0635,  0.0427,  ...,  0.0490,  0.0542,  0.0525]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
